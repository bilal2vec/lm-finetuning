# ToDo

-   How to compute perplexity on specific datasets
-   download datasets
    -   Drive?
-   customize/improve code
    -   check ctrl control codes
-   colab remote tool

# Done

-   look at citations for gpt2
-   Find papers
-   find repositories
-   Read papers
-   get a constant code format
-   summarize major contributions from papers
-   look at notes
-   https://github.com/deepmind/pg19
-   use tokenizers repo
-   deal with multiple files

# Wont do

-   find colab notebooks on finetuning
-   annotate MASS
-   https://colab.research.google.com/drive/1-ROO7L09EupLFLQM-TWgDHa5-FIOdLLh

# Later

-   find old papers
-   Find other guides and papers on how to finetune
-   table to compare papers
-   turing nlg - not available yet
-   think about data that's to large to fit in RAM later
-   tokenize on the fly?
-   TPU support: https://github.com/pytorch/xla/pull/1650
    -   mp still doesn't work
-   update wandb code
