# ToDo

-   find lm trained with unlikelihood objective
-   look at compressive transformer and reformer
-   n_tokens and n_original_tokens not the same when using --fast or --efficient

# Done

-   look at citations for gpt2
-   Find papers
-   find repositories
-   Read papers
-   get a constant code format
-   summarize major contributions from papers
-   look at notes
-   https://github.com/deepmind/pg19
-   use tokenizers repo
-   deal with multiple files
-   customize/improve code
-   How to compute perplexity on specific datasets
-   download datasets
    -   Drive?
-   test model
-   check ppl calc
    -   ppl is correct
    -   check if adjusted ppl is correct

# Wont do

-   find colab notebooks on finetuning
-   annotate MASS
-   https://colab.research.google.com/drive/1-ROO7L09EupLFLQM-TWgDHa5-FIOdLLh
-   colab remote tool
    -   not worth it

# Later

-   find old papers
-   Find other guides and papers on how to finetune
-   table to compare papers
-   turing nlg - not available yet
-   think about data that's to large to fit in RAM later
-   tokenize on the fly?
-   TPU support: https://github.com/pytorch/xla/pull/1650
    -   mp still doesn't work
-   update wandb code
-   fix resuming
    -   has to wait until fasttokenizer is fixed
-   remote debugging
    -   wait until using vms
-   ctrl control codes
    -   prob redo dataset processing to save metadata
