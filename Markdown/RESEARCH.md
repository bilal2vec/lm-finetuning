# Research

## Objective

Find best practices for finetuning language models for text generation.

## What to try

-   context size
-   datasets
    -   large
    -   small
    -   custom
    -   well known
    -   specialized
    -   general
-   lm/pplm
-   loss
-   multiple datasets/control codes
-   evaluation metrics
-   perplexity?
-   other papers
-   dataset sampling strategy
-   optimizers
    -   adagrad
    -   adafactor

## Finetuning Datasets

Some language models might have been pretrained on some of these datasets.

-   IMDB
-   AG News
-   Yahoo answers
-   Wikitext103
-   wikitext2
-   Penn Treebank
-   text8 (Cleaned version of enwiki8)
-   enwiki8 (Looks like this has all the XML?)
-   Project Gutenberg
-   Amazon reviews (McAuley et al, 2015)
-   CNN/Daily Mail (Hermann et al, 2015)
-   Bookscorpus (Aligning books and movies... Zhu et al, 2015)
-   One billion words
-   Yelp reviews (Character level convolutional networks... Zhang et al, 2015)
-   WritingsPrompts (Heiarchichal neural story generation. Fan et al 2018)

### Resources

-   https://course.fast.ai/datasets
-   https://paperswithcode.com/task/language-modelling

## LMs
