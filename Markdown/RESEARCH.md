# Research

## Notes

-   n_tokens and n_original_tokens not the same when using `--fast` or `--efficient`
-   adj ppl not accurate when using `--n_batches` or `--n_tokens`

## Objectives

-   If I wanted to finetune a LM to generate text of a specific style/content, what good defaults would I choose?

-   Find best practices for finetuning tranformer language models for text generation.
-   Understand the "theory" of how language models can be finetuned
-   Present a finetuned LM that can generate coherent text across a range of domains
-   Understand the effect of model and dataset size on generating coherent text
-   Understand the effect of loss and sampling stategies on generating coherent text
-   Present the smallest and most resource efficient LM that can generate coherent text
-   The extent of language models needing to be large to generate coherent text

## Roadmap

-   get initial experiments done on distilgpt2
-   get more datasets
    -   I want a variety of styles, size, doc len, etc
-   do sampling research relatively early on

## Questions

-   datasets
    -   how do you train an lm when dataset is:
    -   large
    -   small
    -   custom
    -   well known
    -   specialized
    -   general
-   is pretraining a lm again ever needed?
    -   does finetuning give better results than grover, ctrl, etc
-   what is the best pretrained lm for text gen?
    -   best for widest range of text gen
    -   best for specific type of text gen (e.g. news, dialog)
    -   can tf-xl be replaced with gpt2 and sliding windows
-   effect of tokenizing approach (line by line, seq_len length chunks, lazy loading with random start point)
    -   might have an effect on efficiency
    -   lazy loading
        -   choose a random start point
        -   get 1k chars from that point
        -   tokenize and discard excess
-   long-form generation with sliding windows
    -   Try leaking data to the lm by training on contigous sequences (move the start position of input sequence 1 step up
-   pplm
    -   generalize pplm
        -   more attribute models for normal generation use
-   loss
    -   unlikelihood training
-   multiple datasets/control codes
    -   use control codes to train on a range of small (?) datasets
    -   is finetuning lm to work with control codes enough? or should it be pretrained with control codes too?
-   models
    -   gpt2
    -   ctrl
    -   grover
    -   electra
    -   distilgpt2
    -   dialogpt2
    -   compressive transformer
    -   reformer
-   objective
    -   mle
    -   electra
    -   unlikelihood

### Things to vary

-   model size
-   batch size
-   context size
-   sampling strategy
    -   how to evaluate?
-   model type

### For making finetuning more efficient

-   optimizers
    -   sgd/momentum
    -   adam/adamw
    -   adafactor
    -   https://github.com/deepmind/lamb
    -   lars?
-   pure fp16 training
-   Compare a small finetuned lm to a larger non-finetuned lm
-   how much worse is text generated by a non-finetuned lm?
-   based on a user's computation budget, should they finetune a small lm or use a large lm
-   tpu bfloat16

### Theory of LMs

-   does a decrease in ppl lead to better text?
-   Can lms where train loss is close to 0 generate text well?
-   see if biases and layernorm should be finetuned
-   evaluation metrics
    -   any better evaluation metrics than ppl
    -   perplexity?
-   double descent
    -   finetune for a long time, does double descent happen when train loss is 0?
    -   anything special about generating text that makes double descent not a good choice?
-   better to train on repeated parts of dataset, or go through a large dataset once

### Hyperparameter tuning

-   learning rate decay strategies
-   warmup
-   effect of gpt-2's gelu and layernorm to inputs

## Finetuning Datasets

Some language models might have been pretrained on some of these datasets.

-   classic lm datasets
    -   Wikitext
        -   from: https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/
        -   People train on raw data then normalize perplexity to match wikitext's tokenization
        -   Wikitext103
            -   ~550MB
            -   100M words
            -   1M sequences of length 256
        -   Wikitext2
            -   ~10MB
            -   2M words
            -   10k sequences of length 256
    -   Penn Treebank
        -   get from https://github.com/salesforce/awd-lstm-lm/blob/master/getdata.sh#L33
        -   ~5MB
        -   1M words
        -   5k sequences of length 256
-   reviews
    -   IMDB (Google Drive)
        -   https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews
        -   https://ai.stanford.edu/~amaas/data/sentiment/
        -   50MB
        -   10M words
        -   50k sequences of length 256
-   books/stories
    -   WritingsPrompts (Heiarchichal neural story generation. Fan et al 2018)
        -   from: https://github.com/pytorch/fairseq/tree/master/examples/stories
        -   paper with metrics: https://arxiv.org/pdf/1805.04833.pdf
        -   paper models only on the first 1k words, but try more
        -   1 line/story
        -   comes with pairs of wp and story
        -   ~900MB
        -   200M words
        -   700k sequences of length 256
    -   PG-19 Language Modelling Benchmark
        -   https://github.com/deepmind/pg19
        -   Stored as a GCP bucket
        -   Train set is 10GB
        -   Val set
            -   50 books
            -   5M words
            -   20k sequences of length 256
        -   Test set
            -   100 books
            -   10M words
            -   40k sequences of length 256
-   news articles
    -   CNN/Daily Mail (Hermann et al, 2015)
        -   raw dataset from: https://cs.nyu.edu/~kcho/DMQA/
        -   Very large: 1.3GB
        -   300M words
        -   10M sequences of length 256
    -   **collect own news dataset**
-   online comments
    -   **find later**

### Won't use:

-   enwiki8 (Looks like this has all the XML? or not)
    -   text8 (Cleaned version of enwiki8)
    -   progress measured in bpc, so probably won't use it
-   Yahoo answers
-   Project Gutenberg
    -   replaced by PG-19
    -   data from: https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html
-   Amazon reviews (McAuley et al, 2015)
    -   won't use
-   Bookscorpus (Aligning books and movies... Zhu et al, 2015)
    -   won't use
-   One billion words
    -   won't use
-   Yelp reviews (Character level convolutional networks... Zhang et al, 2015)
    -   won't use
-   CC-Stories (Trinh and Lee 2018)
    -   not relavent and can't find data
-   AG News
    -   only has title and descriptions
    -   https://course.fast.ai/datasets#nlp
-   OpenWebText (Megatron LM version)
    -   openwebtext repo
    -   newspaper to download text
    -   langdetect to filter content
    -   ftfy for unicode normalization
    -   filter out docs under 128 tokens
    -   lsh to deduplicate content with jaccard similarity more than 0.7
    -   end of text token to end of document
    -   174 GB of text
-   Wikipedia (Devlin et al 2018)
    -   wikitext is a good enough alternative

### Resources

-   https://course.fast.ai/datasets
-   https://paperswithcode.com/task/language-modelling
